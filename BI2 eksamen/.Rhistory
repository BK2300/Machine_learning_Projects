col = "blue", pch = 16, cex = 0.6)
abline(h = 0, col = "red", lwd = 2)
# Plot residualer for Lasso
plot(y_test, lasso_residuals,
xlab = "Faktiske v√¶rdier",
ylab = "Residualer",
main = "Residualplot - Lasso Regression",
col = "green", pch = 16, cex = 0.6)
abline(h = 0, col = "red", lwd = 2)
# Plot residualer for Elastic Net
plot(y_test, elastic_residuals,
xlab = "Faktiske v√¶rdier",
ylab = "Residualer",
main = "Residualplot - Elastic Net",
col = "black", pch = 16, cex = 0.6)
abline(h = 0, col = "red", lwd = 2)
# Plot residualer for Lasso
plot(y_test, lasso_residuals,
xlab = "Faktiske v√¶rdier",
ylab = "Residualer",
main = "Residualplot - Lasso Regression",
col = "green", pch = 16, cex = 0.6)
# Plot residualer for Ridge
plot(y_test, ridge_residuals,
xlab = "Faktiske v√¶rdier",
ylab = "Residualer",
main = "Residualplot - Ridge Regression",
col = "blue", pch = 16, cex = 0.6)
# Ridge regression koefficientsti (alpha = 0)
plot(ridge_model, xvar = "lambda", label = TRUE, main = "Ridge Regression Koefficientsti")
# Lasso regression koefficientsti (alpha = 1)
plot(lasso_model, xvar = "lambda", label = TRUE, main = "Lasso Regression Koefficientsti")
# Plot forudsigelser vs. faktiske v√¶rdier
plot(y_test, elastic_predictions,
xlab = "Faktiske v√¶rdier",
ylab = "Forudsagte v√¶rdier",
main = "Elastic Net: Faktiske vs. Forudsagte v√¶rdier",
col = "black", pch = 16, cex = 0.6)
# Tilf√∏j en r√∏d linje for perfekt forudsigelse
abline(0, 1, col = "red", lwd = 2)
# Ridge regression koefficientsti (alpha = 0)
plot(ridge_model, xvar = "lambda", label = TRUE, main = "Ridge Regression Koefficientsti")
# Lasso regression koefficientsti (alpha = 1)
plot(lasso_model, xvar = "lambda", label = TRUE, main = "Lasso Regression Koefficientsti")
# Find den optimale lambda-v√¶rdi
best_lambda_ridge <- ridge_model$lambda.min
print(paste("Optimal lambda for Ridge:", round(best_lambda_ridge, 4)))
# Beregn MAE for Elastic Net
elastic_mae <- mean(abs(elastic_predictions - y_test))
print(paste("Elastic Net MAE:", round(elastic_mae, 4)))
# Beregn MAE for Ridge Regression
ridge_mae <- mean(abs(ridge_predictions - y_test))
print(paste("Ridge MAE:", round(ridge_mae, 4)))
# Beregn MAE for Lasso Regression
lasso_mae <- mean(abs(lasso_predictions - y_test))
print(paste("Lasso MAE:", round(lasso_mae, 4)))
# Beregn MAE for Elastic Net
elastic_mae <- mean(abs(elastic_predictions - y_test))
print(paste("Elastic Net MAE:", round(elastic_mae, 4)))
# Hent den optimale lambda for hver model
best_lambda_ridge <- ridge_model$lambda.min
best_lambda_lasso <- lasso_model$lambda.min
best_lambda_elastic <- best_lambda
# Udskriv lambda-v√¶rdier
print(paste("Optimal lambda for Ridge:", round(best_lambda_ridge, 4)))
print(paste("Optimal lambda for Lasso:", round(best_lambda_lasso, 4)))
print(paste("Optimal lambda for Elastic Net:", round(best_lambda_elastic, 4)))
# Hent koefficienter fra Lasso-model
lasso_coef <- coef(lasso_model, s = best_lambda_lasso)
# Udskriv variabler med koefficienter ‚â† 0
important_features <- rownames(lasso_coef)[lasso_coef[,1] != 0]
print("Udvalgte features i Lasso:")
print(important_features)
# Hent koefficienter fra Lasso-model
lasso_coef <- coef(lasso_model, s = best_lambda_lasso)
# Udskriv variabler med koefficienter ‚â† 0
important_features <- rownames(lasso_coef)[lasso_coef[,1] != 0]
print("Udvalgte features i Lasso:")
print(important_features)
# Load pakkerne
library(tidyverse)   # Data wrangling, filtrering, visualisering
library(lubridate)   # H√•ndtering af datoer og tid
library(rpart)       # Beslutningstr√¶er
library(rpart.plot)  # Visualisering af beslutningstr√¶er
library(caret)       # Dataopdeling, modeltr√¶ning og evaluering
library(ggplot2)     # Ekstra datavisualisering
# Indl√¶s datas√¶ttet
rental_data <- read_csv(file.choose())
# Tjek de f√∏rste r√¶kker af data
head(rental_data)
# Konverter tekstvariabler til faktorer
rental_data <- rental_data %>%
mutate(
season = as.factor(season),
month = as.factor(month),
weekday = as.factor(weekday),
holiday = as.factor(holiday),
weather = as.factor(weather)
)
# K√∏r summary igen for at se de opdaterede resultater
summary(rental_data)
# Smelt data til lang format for at kunne lave √©t samlet boxplot
rental_data_long <- rental_data %>%
select(fraction, temperature, feelslike, windspeed, humidity) %>%
pivot_longer(cols = everything(), names_to = "Variable", values_to = "Value")
# Lav √©t samlet boxplot
ggplot(rental_data_long, aes(x = Variable, y = Value, fill = Variable)) +
geom_boxplot() +
theme_minimal() +
ggtitle("Boxplots af vigtige numeriske variabler") +
theme(legend.position = "none")  # Skjul legend for et renere plot
ggplot(rental_data, aes(y = fraction)) +
geom_boxplot(fill = "red") +
theme_minimal() +
ggtitle("Boxplot af fraction (andel af lejlighedsvis brugere)")
# Lav histogrammer for de vigtigste numeriske variabler
ggplot(rental_data_long, aes(x = Value, fill = Variable)) +
geom_histogram(bins = 30, alpha = 0.6, position = "identity") +
facet_wrap(~Variable, scales = "free") +
theme_minimal() +
ggtitle("Histogrammer af vigtige numeriske variabler")
# Smelt data for scatterplots (beholder fraction som y)
scatter_data_long <- rental_data %>%
select(fraction, temperature, feelslike, windspeed, humidity) %>%
pivot_longer(cols = -fraction, names_to = "Variable", values_to = "Value")
# Scatterplot af fraction mod de andre variabler
ggplot(scatter_data_long, aes(x = Value, y = fraction, color = Variable)) +
geom_point(alpha = 0.5) +
facet_wrap(~Variable, scales = "free_x") +
theme_minimal() +
ggtitle("Scatterplots: Sammenh√¶ng mellem fraction og vigtige variabler")
# Beregn korrelationen for alle numeriske variabler
cor_matrix <- cor(rental_data %>% select_if(is.numeric), use = "complete.obs")
# Vis korrelationsmatrix
print(cor_matrix)
library(ggcorrplot)
# Plot korrelationsmatrix som heatmap
ggcorrplot(cor_matrix)
## üìå Indl√¶s n√∏dvendige pakker
library(rpart)
library(rpart.plot)
library(DescTools)  # RMSE-beregning
library(dplyr)
library(plotmo)     # Partial Dependence Plots
library(ipred)      # Bagging
library(randomForest) # Random Forest
library(caret)      # Optimering af hyperparametre
## üìå Opdel data i tr√¶ning og test (80% tr√¶ning, 20% test)
set.seed(123)
trainIndex <- createDataPartition(rental_data$fraction, p = 0.8, list = FALSE)
train_data <- rental_data[trainIndex, ]
test_data  <- rental_data[-trainIndex, ]
## üìå Fjern NA-v√¶rdier fra begge datas√¶t
train_data <- na.omit(train_data)
test_data  <- na.omit(test_data)  # Vigtigt at g√∏re det for begge!
## üìå Konverter kategoriske variabler til faktorer (for begge datas√¶t)
train_data <- train_data %>%
mutate(across(c(season, weekday, holiday, weather), as.factor))
test_data <- test_data %>%
mutate(across(c(season, weekday, holiday, weather), as.factor))
## üî• Beslutningstr√¶ uden krydsvalidering üî•
tree_model <- rpart(fraction ~ temperature + feelslike + humidity + windspeed +
season + weekday + holiday + weather + hour,
data = train_data, method = "anova",
control = list(cp = 0.01, maxdepth = 5))
# üìå Find den optimale CP-v√¶rdi og besk√¶r tr√¶et
optimal_cp <- tree_model$cptable[which.min(tree_model$cptable[, "xerror"]), "CP"]
pruned_tree <- prune(tree_model, cp = optimal_cp)
# üìå Visualis√©r det besk√•rne tr√¶
rpart.plot(pruned_tree, type = 3, extra = 101, under = TRUE, tweak = 1.2, main = "Besk√•ret Beslutningstr√¶")
# üìå Partial Dependence Plots
plotmo(pruned_tree, pmethod = "partdep", degree1 = TRUE, main = "Partial Dependence Plots")
# üìå Lav forudsigelser og beregn RMSE for beslutningstr√¶et
predictions <- predict(pruned_tree, newdata = test_data)
rmse_tree <- sqrt(mean((predictions - test_data$fraction)^2, na.rm = TRUE))
print(paste("Beslutningstr√¶ RMSE:", round(rmse_tree, 4)))
# üìå Plot forudsigelser vs. faktiske v√¶rdier for beslutningstr√¶
plot(test_data$fraction, predictions, xlab = "Faktiske v√¶rdier", ylab = "Forudsagte v√¶rdier",
main = "Beslutningstr√¶: Faktiske vs. Forudsagte v√¶rdier")
abline(0,1, col="red")
## üî• Bagging af beslutningstr√¶er üî•
set.seed(123)
bagged_tree_model <- bagging(fraction ~ temperature + feelslike + humidity + windspeed +
season + weekday + holiday + weather + hour,
data = train_data, nbagg = 50)
# üìå Lav forudsigelser og beregn RMSE for bagging
bagging_predictions <- predict(bagged_tree_model, newdata = test_data)
rmse_bagging <- sqrt(mean((bagging_predictions - test_data$fraction)^2))
print(paste("Bagging RMSE:", round(rmse_bagging, 4)))
# üìå Plot forudsigelser vs. faktiske v√¶rdier for bagging
plot(test_data$fraction, bagging_predictions, xlab = "Faktiske v√¶rdier", ylab = "Forudsagte v√¶rdier",
main = "Bagging: Faktiske vs. Forudsagte v√¶rdier")
abline(0,1, col="red")
## üî• Random Forest Model üî•
set.seed(123)
rf_model <- randomForest(fraction ~ temperature + feelslike + humidity + windspeed +
season + weekday + holiday + weather + hour,
data = train_data, ntree = 100, mtry = 3, importance = TRUE)
# üìå Lav forudsigelser og beregn RMSE for Random Forest
rf_predictions <- predict(rf_model, newdata = test_data)
rmse_rf <- sqrt(mean((rf_predictions - test_data$fraction)^2))
print(paste("Random Forest RMSE:", round(rmse_rf, 4))) #(MEGET LANGSOM, TAG EN PAUSE P√Ö 1MIN!)
# üìå Plot forudsigelser vs. faktiske v√¶rdier for Random Forest
plot(test_data$fraction, rf_predictions, xlab = "Faktiske v√¶rdier", ylab = "Forudsagte v√¶rdier",
main = "Random Forest: Faktiske vs. Forudsagte v√¶rdier")
abline(0,1, col="red")
# üìå Visualis√©r vigtighed af variabler
varImpPlot(rf_model)
# üìå Sammenlign RMSE for alle modeller
rmse_results <- data.frame(
Model = c("Beslutningstr√¶", "Bagging", "Random Forest"),
RMSE = c(rmse_tree, rmse_bagging, rmse_rf)
)
print(rmse_results)  # Sammenligning af modelpr√¶cision
# Beregn Mean Squared Error (MSE)
mse_tree <- mean((predictions - test_data$fraction)^2)
# Print MSE-v√¶rdien
print(paste("MSE for beslutningstr√¶:", round(mse_tree, 4)))
# Vis variabelvigtighed (Feature Importance)
feature_importance <- varImp(tree_model, scale = FALSE)
# Print vigtighed for hver feature
print(feature_importance)
# Visualiser variabelvigtighed som en graf
library(ggplot2)
ggplot(feature_importance, aes(x = reorder(rownames(feature_importance), Overall), y = Overall)) +
geom_bar(stat = "identity", fill = "steelblue") +
coord_flip() +
theme_minimal() +
ggtitle("Feature Importance for Beslutningstr√¶")
# Beregn Mean Squared Error (MSE) for beslutningstr√¶et
mse_tree <- mean((predictions - test_data$fraction)^2)
# Beregn MSE for Bagging
mse_bagging <- mean((bagging_predictions - test_data$fraction)^2)
# Beregn MSE for Random Forest
mse_rf <- mean((rf_predictions - test_data$fraction)^2)
# Print alle MSE-v√¶rdierne
print(paste("MSE for beslutningstr√¶:", round(mse_tree, 4)))
print(paste("MSE for Bagging:", round(mse_bagging, 4)))
print(paste("MSE for Random Forest:", round(mse_rf, 4)))
# Saml resultaterne i en tabel for bedre overskuelighed
mse_results <- data.frame(
Model = c("Beslutningstr√¶", "Bagging", "Random Forest"),
MSE = c(mse_tree, mse_bagging, mse_rf)
)
# Vis resultaterne i en tabel
print(mse_results)
# Feature Importance for Beslutningstr√¶
tree_importance <- as.data.frame(varImp(pruned_tree, scale = FALSE))
tree_importance <- tree_importance %>% arrange(desc(Overall))
print("Feature Importance for Beslutningstr√¶:")
print(tree_importance)
# Feature Importance for Bagging
bagging_importance <- as.data.frame(varImp(bagged_tree_model, scale = FALSE))
bagging_importance <- bagging_importance %>% arrange(desc(Overall))
# Feature Importance for Bagging
bagging_importance <- as.data.frame(varImp(bagged_tree_model, scale = FALSE))
bagging_importance <- bagging_importance %>% arrange(desc(Overall))
print("Feature Importance for Bagging:")
print(bagging_importance)
# Feature Importance for Beslutningstr√¶
tree_importance <- as.data.frame(varImp(pruned_tree, scale = FALSE))
tree_importance <- tree_importance %>% arrange(desc(Overall))
print("Feature Importance for Beslutningstr√¶:")
print(tree_importance)
# Feature Importance for Bagging
bagging_importance <- as.data.frame(varImp(bagged_tree_model, scale = FALSE))
bagging_importance <- bagging_importance %>% arrange(desc(Overall))
print("Feature Importance for Bagging:")
print(bagging_importance)
# Feature Importance for Random Forest
rf_importance <- as.data.frame(importance(rf_model))
rf_importance$Variable <- rownames(rf_importance) # Tilf√∏j variabelnavne
rf_importance <- rf_importance %>% arrange(desc(IncNodePurity)) # Sorter efter betydning
print("Feature Importance for Random Forest:")
print(rf_importance)
# Saml resultaterne i en liste for bedre overskuelighed
feature_importance_results <- list(
Beslutningstr√¶ = tree_importance,
Bagging = bagging_importance,
RandomForest = rf_importance
)
# Vis resultaterne
feature_importance_results
# Saml resultaterne i en liste for bedre overskuelighed
feature_importance_results <- list(
Beslutningstr√¶ = tree_importance,
Bagging = bagging_importance,
RandomForest = rf_importance
)
# Feature Importance for Random Forest
rf_importance <- as.data.frame(importance(rf_model))
rf_importance$Variable <- rownames(rf_importance) # Tilf√∏j variabelnavne
rf_importance <- rf_importance %>% arrange(desc(IncNodePurity)) # Sorter efter betydning
print("Feature Importance for Random Forest:")
print(rf_importance)
# Feature Importance for Random Forest
rf_importance <- as.data.frame(importance(rf_model))
rf_importance$Variable <- rownames(rf_importance) # Tilf√∏j variabelnavne
rf_importance <- rf_importance %>% arrange(desc(IncNodePurity)) # Sorter efter betydning
print("Feature Importance for Random Forest:")
print(rf_importance)
# Visualiser Feature Importance for Random Forest
library(ggplot2)
ggplot(rf_importance, aes(x = reorder(Variable, IncNodePurity), y = IncNodePurity)) +
geom_bar(stat = "identity", fill = "steelblue") +
coord_flip() +
theme_minimal() +
labs(title = "Feature Importance for Random Forest",
x = "Variabler", y = "Betydning (IncNodePurity)")
# Vis resultaterne i en tabel
print(mse_results)
# Load pakkerne
library(tidyverse)   # Data wrangling, filtrering, visualisering
library(lubridate)   # H√•ndtering af datoer og tid
library(rpart)       # Beslutningstr√¶er
library(rpart.plot)  # Visualisering af beslutningstr√¶er
library(caret)       # Dataopdeling, modeltr√¶ning og evaluering
library(ggplot2)     # Ekstra datavisualisering
# Indl√¶s datas√¶ttet
rental_data <- read_csv(file.choose())
# Indl√¶s datas√¶ttet
rental_data <- read_csv(file.choose())
# Tjek de f√∏rste r√¶kker af data
head(rental_data)
# Konverter tekstvariabler til faktorer
rental_data <- rental_data %>%
mutate(
season = as.factor(season),
month = as.factor(month),
weekday = as.factor(weekday),
holiday = as.factor(holiday),
weather = as.factor(weather)
)
# K√∏r summary igen for at se de opdaterede resultater
summary(rental_data)
# Smelt data til lang format for at kunne lave √©t samlet boxplot
rental_data_long <- rental_data %>%
select(fraction, temperature, feelslike, windspeed, humidity) %>%
pivot_longer(cols = everything(), names_to = "Variable", values_to = "Value")
# Lav √©t samlet boxplot
ggplot(rental_data_long, aes(x = Variable, y = Value, fill = Variable)) +
geom_boxplot() +
theme_minimal() +
ggtitle("Boxplots af vigtige numeriske variabler") +
theme(legend.position = "none")  # Skjul legend for et renere plot
ggplot(rental_data, aes(y = fraction)) +
geom_boxplot(fill = "red") +
theme_minimal() +
ggtitle("Boxplot af fraction (andel af lejlighedsvis brugere)")
# Lav histogrammer for de vigtigste numeriske variabler
ggplot(rental_data_long, aes(x = Value, fill = Variable)) +
geom_histogram(bins = 30, alpha = 0.6, position = "identity") +
facet_wrap(~Variable, scales = "free") +
theme_minimal() +
ggtitle("Histogrammer af vigtige numeriske variabler")
# Smelt data for scatterplots (beholder fraction som y)
scatter_data_long <- rental_data %>%
select(fraction, temperature, feelslike, windspeed, humidity) %>%
pivot_longer(cols = -fraction, names_to = "Variable", values_to = "Value")
# Scatterplot af fraction mod de andre variabler
ggplot(scatter_data_long, aes(x = Value, y = fraction, color = Variable)) +
geom_point(alpha = 0.5) +
facet_wrap(~Variable, scales = "free_x") +
theme_minimal() +
ggtitle("Scatterplots: Sammenh√¶ng mellem fraction og vigtige variabler")
# Beregn korrelationen for alle numeriske variabler
cor_matrix <- cor(rental_data %>% select_if(is.numeric), use = "complete.obs")
# Vis korrelationsmatrix
print(cor_matrix)
library(ggcorrplot)
# Plot korrelationsmatrix som heatmap
ggcorrplot(cor_matrix)
## üìå Indl√¶s n√∏dvendige pakker
library(rpart)
library(rpart.plot)
library(DescTools)  # RMSE-beregning
library(dplyr)
library(plotmo)     # Partial Dependence Plots
library(ipred)      # Bagging
library(randomForest) # Random Forest
library(caret)      # Optimering af hyperparametre
## üìå Opdel data i tr√¶ning og test (80% tr√¶ning, 20% test)
set.seed(123)
trainIndex <- createDataPartition(rental_data$fraction, p = 0.8, list = FALSE)
train_data <- rental_data[trainIndex, ]
test_data  <- rental_data[-trainIndex, ]
## üìå Fjern NA-v√¶rdier fra begge datas√¶t
train_data <- na.omit(train_data)
test_data  <- na.omit(test_data)  # Vigtigt at g√∏re det for begge!
## üìå Konverter kategoriske variabler til faktorer (for begge datas√¶t)
train_data <- train_data %>%
mutate(across(c(season, weekday, holiday, weather), as.factor))
test_data <- test_data %>%
mutate(across(c(season, weekday, holiday, weather), as.factor))
## üî• Beslutningstr√¶ uden krydsvalidering üî•
tree_model <- rpart(fraction ~ temperature + feelslike + humidity + windspeed +
season + weekday + holiday + weather + hour,
data = train_data, method = "anova",
control = list(cp = 0.01, maxdepth = 5))
# üìå Find den optimale CP-v√¶rdi og besk√¶r tr√¶et
optimal_cp <- tree_model$cptable[which.min(tree_model$cptable[, "xerror"]), "CP"]
pruned_tree <- prune(tree_model, cp = optimal_cp)
# üìå Visualis√©r det besk√•rne tr√¶
rpart.plot(pruned_tree, type = 3, extra = 101, under = TRUE, tweak = 1.2, main = "Besk√•ret Beslutningstr√¶")
# üìå Partial Dependence Plots
plotmo(pruned_tree, pmethod = "partdep", degree1 = TRUE, main = "Partial Dependence Plots")
# üìå Lav forudsigelser og beregn RMSE for beslutningstr√¶et
predictions <- predict(pruned_tree, newdata = test_data)
rmse_tree <- sqrt(mean((predictions - test_data$fraction)^2, na.rm = TRUE))
print(paste("Beslutningstr√¶ RMSE:", round(rmse_tree, 4)))
# üìå Plot forudsigelser vs. faktiske v√¶rdier for beslutningstr√¶
plot(test_data$fraction, predictions, xlab = "Faktiske v√¶rdier", ylab = "Forudsagte v√¶rdier",
main = "Beslutningstr√¶: Faktiske vs. Forudsagte v√¶rdier")
abline(0,1, col="red")
## üî• Bagging af beslutningstr√¶er üî•
set.seed(123)
bagged_tree_model <- bagging(fraction ~ temperature + feelslike + humidity + windspeed +
season + weekday + holiday + weather + hour,
data = train_data, nbagg = 50)
# üìå Lav forudsigelser og beregn RMSE for bagging
bagging_predictions <- predict(bagged_tree_model, newdata = test_data)
rmse_bagging <- sqrt(mean((bagging_predictions - test_data$fraction)^2))
print(paste("Bagging RMSE:", round(rmse_bagging, 4)))
# üìå Plot forudsigelser vs. faktiske v√¶rdier for bagging
plot(test_data$fraction, bagging_predictions, xlab = "Faktiske v√¶rdier", ylab = "Forudsagte v√¶rdier",
main = "Bagging: Faktiske vs. Forudsagte v√¶rdier")
abline(0,1, col="red")
## üî• Random Forest Model üî•
set.seed(123)
rf_model <- randomForest(fraction ~ temperature + feelslike + humidity + windspeed +
season + weekday + holiday + weather + hour,
data = train_data, ntree = 100, mtry = 3, importance = TRUE)
# üìå Lav forudsigelser og beregn RMSE for Random Forest
rf_predictions <- predict(rf_model, newdata = test_data)
rmse_rf <- sqrt(mean((rf_predictions - test_data$fraction)^2))
print(paste("Random Forest RMSE:", round(rmse_rf, 4))) #(MEGET LANGSOM, TAG EN PAUSE P√Ö 1MIN!)
# üìå Plot forudsigelser vs. faktiske v√¶rdier for Random Forest
plot(test_data$fraction, rf_predictions, xlab = "Faktiske v√¶rdier", ylab = "Forudsagte v√¶rdier",
main = "Random Forest: Faktiske vs. Forudsagte v√¶rdier")
abline(0,1, col="red")
# üìå Visualis√©r vigtighed af variabler
varImpPlot(rf_model)
# üìå Sammenlign RMSE for alle modeller
rmse_results <- data.frame(
Model = c("Beslutningstr√¶", "Bagging", "Random Forest"),
RMSE = c(rmse_tree, rmse_bagging, rmse_rf)
)
print(rmse_results)  # Sammenligning af modelpr√¶cision
# Beregn Mean Squared Error (MSE) for beslutningstr√¶et
mse_tree <- mean((predictions - test_data$fraction)^2)
# Beregn MSE for Bagging
mse_bagging <- mean((bagging_predictions - test_data$fraction)^2)
# Beregn MSE for Random Forest
mse_rf <- mean((rf_predictions - test_data$fraction)^2)
# Print alle MSE-v√¶rdierne
print(paste("MSE for beslutningstr√¶:", round(mse_tree, 4)))
print(paste("MSE for Bagging:", round(mse_bagging, 4)))
print(paste("MSE for Random Forest:", round(mse_rf, 4)))
# Saml resultaterne i en tabel for bedre overskuelighed
mse_results <- data.frame(
Model = c("Beslutningstr√¶", "Bagging", "Random Forest"),
MSE = c(mse_tree, mse_bagging, mse_rf)
)
# Vis resultaterne i en tabel
print(mse_results)
# Vis resultaterne i en tabel
print(mse_results)
# Feature Importance for Random Forest
rf_importance <- as.data.frame(importance(rf_model))
rf_importance$Variable <- rownames(rf_importance) # Tilf√∏j variabelnavne
rf_importance <- rf_importance %>% arrange(desc(IncNodePurity)) # Sorter efter betydning
print("Feature Importance for Random Forest:")
print(rf_importance)
# Visualiser Feature Importance for Random Forest
library(ggplot2)
ggplot(rf_importance, aes(x = reorder(Variable, IncNodePurity), y = IncNodePurity)) +
geom_bar(stat = "identity", fill = "steelblue") +
coord_flip() +
theme_minimal() +
labs(title = "Feature Importance for Random Forest",
x = "Variabler", y = "Betydning (IncNodePurity)")
######################################################################################
#MAE
# Load n√∏dvendig pakke
library(Metrics)  # Indeholder mae()-funktionen
# Beregn MAE for beslutningstr√¶et
mae_tree <- mae(test_data$fraction, predictions)
install.packages("Metrics")
######################################################################################
#MAE
# Load n√∏dvendig pakke
library(Metrics)  # Indeholder mae()-funktionen
######################################################################################
#MAE
# Load n√∏dvendig pakke
library(Metrics)  # Indeholder mae()-funktionen
# Beregn MAE for beslutningstr√¶et
mae_tree <- mae(test_data$fraction, predictions)
print(paste("MAE for Beslutningstr√¶:", round(mae_tree, 4)))
# Beregn MAE for Bagging
mae_bagging <- mae(test_data$fraction, bagging_predictions)
print(paste("MAE for Bagging:", round(mae_bagging, 4)))
# Beregn MAE for Random Forest
mae_rf <- mae(test_data$fraction, rf_predictions)
print(paste("MAE for Random Forest:", round(mae_rf, 4)))
# Saml resultaterne i en tabel
mae_results <- data.frame(
Model = c("Beslutningstr√¶", "Bagging", "Random Forest"),
MAE = c(mae_tree, mae_bagging, mae_rf)
)
# Vis resultaterne
print(mae_results)
